## Bias

### Meaning and examples of biases

Biases, generally, are systematic prejudices of a model towards certain outcomes.
Humans make frequent use of biases to function in a complex world with limited cognitive resources.
Our brain seems predisposed to doing causal inference, a skill which we learn and hone from a very early age [@doi:10.1037/0033-295X.111.1.3].
In fact, we may be over-eager to deduce causality from observation (i.e., “jump to conclusions”), which is indicative of a strong inductive bias.
A good *heuristic* is the application of a suitable bias to a problem, such that the solution can be considered acceptable despite limited resources.

In machine learning (ML), we can distinguish between useful and harmful biases.
Harmful biases are common issues in the technical process of training models; they include, for instance, sampling bias, selection bias, confirmation bias, overfitting, and underfitting [].
While addressing harmful biases is a crucial part of ML, we will not discuss them further in this perspective.

Useful biases, on the other hand, are biases that are introduced into a model to improve its performance.
They can be relatively implicit, such as the choice of algorithm, architecture, or regularisation; or explicit, such as the choice of prior knowledge and how it is used.
In the context of CR, useful biases are those that improve the performance of the model in terms of its ability to draw correct causal conclusions.
Since most models developed in biomedical research and the broader ML community are inductive models, one of the most discussed useful biases is *inductive bias*.

### Why do we need biases?

In biomedical research, we operate in a space that is very constrained in the amount and quality of data we can collect.
This is due to the high cost of experiments, the limited availability of samples, and the high dimensionality of the data.
These issues, in combination with the naturally high variability of biological measurements, lead to a relatively low signal-to-noise ratio of our observations.
In addition, we are often trying to "climb" the ladder of causality with our CR approaches, which comes with additional data requirements.
Lastly, we also lack a ground truth for most contexts in which we perform measurements.
As a result, we need to introduce biases into our models to make the most of the data we have.

Some central questions then arise: 

- How do we choose the right biases to introduce?

- How explicit should we be in introducing biases (i.e., should the model determine its own biases, or do we force them on the model)?

- How do we evaluate the biases we introduce?
