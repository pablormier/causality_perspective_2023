## Causality in foundation models

There has been an enormous spike of interest in attention-based neural network models, in large part due to the success of transformers in natural language processing and the commercial acclaim of ChatGPT.
While the high performance of Large Language Models (LLMs) is based on a myriad architectural improvements, the introduction of attention as an architectural bias has been a major contributor to their success [@doi:10.48550/arXiv.1706.03762].
This has led to the development of attention-based molecular models (most commonly for gene expression), which can also be considered "GPT" models: Generative Pre-trained Transformers [@doi:10.1038/s41592-021-01252-x; @doi:10.1038/s41586-023-06139-9; @doi:10.1101/2023.04.30.538439].

Attention as a learning mechanism enables the integration of non-local information in a flexible manner.
In a molecular model that reasons about gene expression, such as Geneformer, attention allows the integration of chromosomally distant regulatory elements [@doi:10.1038/s41586-023-06139-9].
For example, the angiotensin converting enzyme (ACE), which is responsible for converting angiotensin I to angiotensin II, is causally responsible for the activation of the angiotensin receptor II (AGTR2).
However, the ACE gene is located on chromosome 17, while the AGTR2 gene is located on chromosome X.
Thus, to learn the causal relationship between ACE and AGTR2 in a self-supervised manner based purely on observational data, the model must be able to integrate information from distant genomic regions.

The generalist capabilities of LLMs have led to the designation of "foundation models" (cite stanford paper).
Foundation models are models that achieve high performance by the combination of large amounts of data and model parameters, a generic architecture without specific biases, and self-supervised training.
They can be fine-tuned for more specific tasks, because they are thought to derive generalisable representations and mechanisms by training on an amount of data large enough to encapsulate the complexity of real-world systems.
While this designation is not too far off the mark for LLMs, it is not yet clear whether the same can be said for molecular models.

Recent molecular foundation model benchmarks highlight clear discrepancies between the "foundational" aspirations of the pre-trained models and the real-world evaluation of their performance [@doi:10.1101/2023.10.16.561085; @doi:10.1101/2023.10.19.563100].
- Details here

### Attention - and large amounts of data - is all you need?

Given enough data to train on - and ample funds for compute - is attention “all you need” to induce reliable biases in your model?
While there are doubts as regards the reasoning capabilities of our most advanced LLMs, GPT arguably “understands” language very well already, to the point where it can flawlessly communicate and also synthesise information [@doi:10.1038/d41586-023-02361-7].
This is what the term "foundation model" implies: the model has derived a generalisable representation of language, a tool that can be fine-tuned for a variety of language-related tasks.
This behaviour is not possible without assuming some form of causality, even if it is not explicitly encoded in the model.
In this light, what are the reasons to be sceptical about the capacity of molecular foundation models to understand the "grammar" of the cell?

For one, large transformer models are not explainable due to their large number of parameters and non-linearities.
As such, there is no way to scrutinise their reasoning beyond the output they produce.
What seems simple in the case of language models - the famous Turing test can be performed by any human with a basic understanding of language - is exceedingly difficult in the molecular space, where many causal relationships are yet unknown [@doi:10.1038/d41586-023-02361-7].
Yet the only way to scrutinise and subsequently improve the reasoning capabilities of a model is precisely this explicit validation of its predictions in an interpretable setting.

While the creation of explicit molecular models (e.g., logic, structural causal, or ODE-based models) and the self-supervised training of molecular foundation models are methodically very different, both result in a hypothesis on causal structure that can be formulated as a network.
Theodoris et al. explore the attention layers of their Geneformer foundation model to explain the model's reasoning [@doi:10.1038/s41586-023-06139-9].
While some layers show clear patterns of attention, such as attending to highly connected or highly expressed genes, other layers are not as readily interpretable.
Whether these complex layers reflect the true complexity of the underlying biology or are rather evidence for overfitting to the training data is not clear.
One argument in favour of overfitting is the poor generalisation of the model in independent benchmarks [@doi:10.1101/2023.10.16.561085; @doi:10.1101/2023.10.19.563100].
To determine whether molecular foundation models indeed capture generalisable causal representations of biology, dedicated benchmarks are needed.

Cite some benchmarks, point out the most important aspects of benchmarking causally.

What is the mathematical relationship between explicit (e.g. ODE) and implicit (transformers) models? How could this be studied empirically?

Back-of-the-envelope data requirements for molecular foundation models (parameters, data points, comparison with LLMs).

### Causal latent spaces

Latent encodings of explicit prior knowledge (GEARS)

Do 'causal latent spaces' exist, and how would we prove it?

How do we explore these latent spaces and use them for inference?

Stefan's comment: newer architectures (self-supervised) do not decode; how important is it for biological insights, particularly compared with scaling? Exploring and explaining the latent space...

- https://scholar.google.de/citations?view_op=view_citation&hl=de&user=soxv0s0AAAAJ&sortby=pubdate&citation_for_view=soxv0s0AAAAJ:2osOgNQ5qMEC (decoder important)
- https://proceedings.neurips.cc/paper_files/paper/2022/hash/87213955efbe48b46586e37bf2f1fe5b-Abstract-Conference.html (decoder not important)
